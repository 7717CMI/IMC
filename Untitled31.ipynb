{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YOxzY5jLwrk",
        "outputId": "fa30a90d-ac85-4ea1-8e0f-01da10061e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Google Trends Keyword Analyzer\n",
            "==================================================\n",
            "Enter your SerpAPI key: 6c9ca2483cb494765ee7dd42c784388cd540dc50355cc39046b200ea99291784\n",
            "\n",
            "Enter keywords to analyze (comma-separated):\n",
            "> machine learning, iraq\n",
            "\n",
            "📊 Analyzing 2 keywords...\n",
            "📋 Criteria:\n",
            "   • Interest >50 at least twice in 7 days\n",
            "   • USA, Germany, Japan, UK, or France in top 10 countries\n",
            "------------------------------------------------------------\n",
            "\n",
            "[1/2] Processing: machine learning\n",
            "Analyzing: machine learning\n",
            "  Threshold check (>50 twice): True\n",
            "  Target countries check: True\n",
            "  Interest pattern: 54 → 53 → 52 → 54 → 52 → 44 → 47 → 50 → 54 → 59 → 74 → 82 → 84 → 80 → 86 → 97 → 88 → 72 → 72 → 66 → 66 → 65 → 58 → 63 → 55 → 60 → 49 → 49 → 45 → 40 → 39 → 45 → 43 → 52 → 57 → 66 → 74 → 75 → 75 → 70 → 71 → 60 → 61 → 80 → 55 → 73 → 77 → 80 → 69 → 47 → 41 → 41 → 35 → 35 → 33 → 35 → 42 → 49 → 60 → 77 → 80 → 78 → 86 → 70 → 70 → 76 → 100 → 57 → 44 → 49 → 46 → 41 → 41 → 36 → 37 → 29 → 31 → 29 → 29 → 34 → 33 → 38 → 46 → 54 → 57 → 60 → 58 → 48 → 54 → 53 → 50 → 48 → 51 → 53 → 52 → 52 → 50 → 41 → 37 → 38 → 36 → 37 → 37 → 43 → 48 → 56 → 61 → 74 → 78 → 85 → 78 → 71 → 75 → 69 → 61 → 58 → 59 → 63 → 63 → 59 → 58 → 54 → 48 → 44 → 44 → 45 → 41 → 47 → 62 → 61 → 75 → 70 → 79 → 77 → 79 → 82 → 88 → 64 → 59 → 58 → 63 → 80 → 66 → 59 → 56 → 50 → 49 → 48 → 47 → 40 → 40 → 44 → 45 → 51 → 64 → 76 → 79 → 75 → 81 → 77 → 88 → 76 → 63 → 76 → 64 → 61 → 59 → 68 → 60\n",
            "✅ machine learning qualifies!\n",
            "\n",
            "[2/2] Processing: iraq\n",
            "Analyzing: iraq\n",
            "  Threshold check (>50 twice): True\n",
            "  Target countries check: False\n",
            "  Interest pattern: 39 → 45 → 45 → 46 → 48 → 74 → 57 → 42 → 39 → 38 → 35 → 34 → 36 → 34 → 32 → 30 → 29 → 29 → 30 → 29 → 28 → 26 → 29 → 28 → 28 → 30 → 29 → 31 → 30 → 31 → 60 → 100 → 84 → 77 → 63 → 62 → 57 → 57 → 53 → 50 → 54 → 52 → 51 → 53 → 50 → 57 → 51 → 54 → 66 → 73 → 71 → 72 → 72 → 87 → 73 → 68 → 65 → 62 → 64 → 57 → 60 → 54 → 54 → 53 → 50 → 51 → 51 → 48 → 56 → 49 → 52 → 52 → 53 → 56 → 55 → 63 → 58 → 56 → 56 → 57 → 52 → 53 → 58 → 55 → 50 → 52 → 58 → 51 → 50 → 54 → 51 → 54 → 52 → 55 → 55 → 54 → 53 → 53 → 50 → 65 → 60 → 58 → 51 → 52 → 54 → 51 → 55 → 54 → 48 → 48 → 49 → 46 → 47 → 45 → 49 → 40 → 42 → 50 → 49 → 48 → 50 → 48 → 52 → 55 → 57 → 60 → 58 → 60 → 62 → 62 → 57 → 57 → 53 → 50 → 49 → 50 → 52 → 52 → 50 → 50 → 65 → 57 → 59 → 69 → 67 → 64 → 68 → 75 → 73 → 70 → 65 → 68 → 70 → 68 → 66 → 63 → 57 → 55 → 55 → 52 → 52 → 57 → 53 → 55 → 56 → 61 → 62 → 68 → 75\n",
            "❌ iraq doesn't meet criteria\n",
            "\n",
            "🎯 FINAL RESULTS: 1 out of 2 keywords qualify\n",
            "============================================================\n",
            "\n",
            "📈 [1] MACHINE LEARNING\n",
            "   📊 Max Interest: 100%\n",
            "   📊 Avg Interest: 58%\n",
            "   🔥 Days >50: 109\n",
            "   📅 7-day pattern: 54 → 53 → 52 → 54 → 52 → 44 → 47 → 50 → 54 → 59 → 74 → 82 → 84 → 80 → 86 → 97 → 88 → 72 → 72 → 66 → 66 → 65 → 58 → 63 → 55 → 60 → 49 → 49 → 45 → 40 → 39 → 45 → 43 → 52 → 57 → 66 → 74 → 75 → 75 → 70 → 71 → 60 → 61 → 80 → 55 → 73 → 77 → 80 → 69 → 47 → 41 → 41 → 35 → 35 → 33 → 35 → 42 → 49 → 60 → 77 → 80 → 78 → 86 → 70 → 70 → 76 → 100 → 57 → 44 → 49 → 46 → 41 → 41 → 36 → 37 → 29 → 31 → 29 → 29 → 34 → 33 → 38 → 46 → 54 → 57 → 60 → 58 → 48 → 54 → 53 → 50 → 48 → 51 → 53 → 52 → 52 → 50 → 41 → 37 → 38 → 36 → 37 → 37 → 43 → 48 → 56 → 61 → 74 → 78 → 85 → 78 → 71 → 75 → 69 → 61 → 58 → 59 → 63 → 63 → 59 → 58 → 54 → 48 → 44 → 44 → 45 → 41 → 47 → 62 → 61 → 75 → 70 → 79 → 77 → 79 → 82 → 88 → 64 → 59 → 58 → 63 → 80 → 66 → 59 → 56 → 50 → 49 → 48 → 47 → 40 → 40 → 44 → 45 → 51 → 64 → 76 → 79 → 75 → 81 → 77 → 88 → 76 → 63 → 76 → 64 → 61 → 59 → 68 → 60\n",
            "   🌍 Top countries: China, St. Helena, Ethiopia, India, Singapore\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "class GoogleTrendsAgent:\n",
        "    def __init__(self, serp_api_key):\n",
        "        self.api_key = serp_api_key\n",
        "        self.base_url = \"https://serpapi.com/search.json\"\n",
        "\n",
        "    def get_trends_data(self, keyword):\n",
        "        \"\"\"Get interest over time for a keyword\"\"\"\n",
        "        params = {\n",
        "            'engine': 'google_trends',\n",
        "            'q': keyword,\n",
        "            'data_type': 'TIMESERIES',\n",
        "            'date': 'now 7-d',  # Last 7 days\n",
        "            'geo': '',  # Worldwide\n",
        "            'api_key': self.api_key\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'error' in data:\n",
        "                print(f\"API Error for {keyword}: {data['error']}\")\n",
        "                return []\n",
        "\n",
        "            # Correct path based on API documentation\n",
        "            return data.get('interest_over_time', {}).get('timeline_data', [])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching trends for {keyword}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_region_data(self, keyword):\n",
        "        \"\"\"Get interest by region for a keyword\"\"\"\n",
        "        params = {\n",
        "            'engine': 'google_trends',\n",
        "            'q': keyword,\n",
        "            'data_type': 'GEO_MAP_0',  # Correct data_type for interest by region\n",
        "            'date': 'now 7-d',\n",
        "            'api_key': self.api_key\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Check for API errors\n",
        "            if 'error' in data:\n",
        "                print(f\"API Error for {keyword} regions: {data['error']}\")\n",
        "                return []\n",
        "\n",
        "            # Correct path based on API documentation\n",
        "            return data.get('interest_by_region', [])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching regions for {keyword}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def check_threshold_criteria(self, interest_values):\n",
        "        \"\"\"Check if interest > 50 appears at least twice in 7 days\"\"\"\n",
        "        if not interest_values:\n",
        "            return False\n",
        "        days_above_50 = sum(1 for value in interest_values if value and value > 50)\n",
        "        return days_above_50 >= 2\n",
        "\n",
        "    def check_country_criteria(self, region_data):\n",
        "        \"\"\"Check if USA, Germany, Japan, UK, or France are in top countries\"\"\"\n",
        "        if not region_data:\n",
        "            return False\n",
        "\n",
        "        target_countries = ['United States', 'Germany', 'Japan', 'United Kingdom', 'France',\"China\"]\n",
        "        top_countries = [item.get('location', '') for item in region_data[:10]]\n",
        "        return any(country in top_countries for country in target_countries)\n",
        "\n",
        "    def extract_interest_values(self, trends_data, keyword):\n",
        "        \"\"\"Extract interest values from trends data for a single keyword\"\"\"\n",
        "        interest_values = []\n",
        "\n",
        "        for item in trends_data:\n",
        "            values = item.get('values', [])\n",
        "            if values:\n",
        "                # For single keyword queries, there should be one value per time period\n",
        "                if len(values) == 1:\n",
        "                    interest_values.append(values[0].get('extracted_value', 0))\n",
        "                else:\n",
        "                    # If multiple values, find the one for our keyword\n",
        "                    for value in values:\n",
        "                        if value.get('query', '').lower() == keyword.lower():\n",
        "                            interest_values.append(value.get('extracted_value', 0))\n",
        "                            break\n",
        "                    else:\n",
        "                        # If keyword not found, append 0\n",
        "                        interest_values.append(0)\n",
        "\n",
        "        return interest_values\n",
        "\n",
        "    def analyze_keyword(self, keyword):\n",
        "        \"\"\"Analyze a single keyword\"\"\"\n",
        "        print(f\"Analyzing: {keyword}\")\n",
        "\n",
        "        # Get trends data\n",
        "        trends_data = self.get_trends_data(keyword)\n",
        "        if not trends_data:\n",
        "            print(f\"No trends data for {keyword}\")\n",
        "            return None\n",
        "\n",
        "        # Extract interest values\n",
        "        interest_values = self.extract_interest_values(trends_data, keyword)\n",
        "\n",
        "        if len(interest_values) < 7:\n",
        "            print(f\"Not enough data for {keyword} (got {len(interest_values)} data points)\")\n",
        "            return None\n",
        "\n",
        "        # Get region data with rate limiting\n",
        "        time.sleep(1)  # Rate limiting between API calls\n",
        "        region_data = self.get_region_data(keyword)\n",
        "\n",
        "        # Apply your logic\n",
        "        meets_threshold = self.check_threshold_criteria(interest_values)\n",
        "        has_target_countries = self.check_country_criteria(region_data)\n",
        "\n",
        "        print(f\"  Threshold check (>50 twice): {meets_threshold}\")\n",
        "        print(f\"  Target countries check: {has_target_countries}\")\n",
        "        print(f\"  Interest pattern: {' → '.join(map(str, interest_values))}\")\n",
        "\n",
        "        if meets_threshold and has_target_countries:\n",
        "            return {\n",
        "                'keyword': keyword,\n",
        "                'interest_values': interest_values,\n",
        "                'max_interest': max(interest_values) if interest_values else 0,\n",
        "                'avg_interest': round(sum(interest_values) / len(interest_values)) if interest_values else 0,\n",
        "                'days_above_50': sum(1 for v in interest_values if v and v > 50),\n",
        "                'top_countries': [item.get('location', '') for item in region_data[:5]]\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_keywords(self, keywords):\n",
        "        \"\"\"Process list of keywords and return only those meeting criteria\"\"\"\n",
        "        qualifying_keywords = []\n",
        "\n",
        "        for i, keyword in enumerate(keywords):\n",
        "            keyword = keyword.strip()\n",
        "            if not keyword:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n[{i+1}/{len(keywords)}] Processing: {keyword}\")\n",
        "\n",
        "            result = self.analyze_keyword(keyword)\n",
        "            if result:\n",
        "                qualifying_keywords.append(result)\n",
        "                print(f\"✅ {keyword} qualifies!\")\n",
        "            else:\n",
        "                print(f\"❌ {keyword} doesn't meet criteria\")\n",
        "\n",
        "            # Rate limiting between keywords\n",
        "            if i < len(keywords) - 1:  # Don't sleep after the last keyword\n",
        "                time.sleep(2)\n",
        "\n",
        "        return qualifying_keywords\n",
        "\n",
        "def main():\n",
        "    print(\"🔍 Google Trends Keyword Analyzer\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Your SERP API key\n",
        "    API_KEY = input(\"Enter your SerpAPI key: \").strip()\n",
        "\n",
        "    if not API_KEY:\n",
        "        print(\"❌ API key is required!\")\n",
        "        return\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = GoogleTrendsAgent(API_KEY)\n",
        "\n",
        "    # Input keywords\n",
        "    print(\"\\nEnter keywords to analyze (comma-separated):\")\n",
        "    keywords_input = input(\"> \").strip()\n",
        "\n",
        "    if not keywords_input:\n",
        "        print(\"❌ No keywords provided!\")\n",
        "        return\n",
        "\n",
        "    keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]\n",
        "\n",
        "    print(f\"\\n📊 Analyzing {len(keywords)} keywords...\")\n",
        "    print(\"📋 Criteria:\")\n",
        "    print(\"   • Interest >50 at least twice in 7 days\")\n",
        "    print(\"   • USA, Germany, Japan, UK, or France in top 10 countries\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Process keywords\n",
        "    results = agent.process_keywords(keywords)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n🎯 FINAL RESULTS: {len(results)} out of {len(keywords)} keywords qualify\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if results:\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"\\n📈 [{i}] {result['keyword'].upper()}\")\n",
        "            print(f\"   📊 Max Interest: {result['max_interest']}%\")\n",
        "            print(f\"   📊 Avg Interest: {result['avg_interest']}%\")\n",
        "            print(f\"   🔥 Days >50: {result['days_above_50']}\")\n",
        "            print(f\"   📅 7-day pattern: {' → '.join(map(str, result['interest_values']))}\")\n",
        "            print(f\"   🌍 Top countries: {', '.join(result['top_countries'])}\")\n",
        "    else:\n",
        "        print(\"❌ No keywords met the criteria.\")\n",
        "        print(\"\\n💡 Tips:\")\n",
        "        print(\"   • Try more popular/trending keywords\")\n",
        "        print(\"   • Check if keywords have sufficient search volume\")\n",
        "        print(\"   • Verify your API key has sufficient credits\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AylUzRjJL2Vg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}